{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../m6anet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to extract the sample files and run dataprep on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_data/\n",
      "demo_data/eventalign.txt\n",
      "demo_data/summary.txt\n",
      "eventalign.hdf5\n",
      "eventalign.log\n",
      "eventalign.txt\n",
      "inference\n",
      "prepare_for_inference.log\n",
      "summary.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar -xvf ../demo_data.tar.gz -C ../\n",
    "m6anet-dataprep --eventalign ../demo_data/eventalign.txt --summary ../demo_data/summary.txt --out_dir ../demo_data\n",
    "ls ../demo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting ../demo_data.tar.gz gives us eventalign.txt and summary.txt, which are the output files from nanopolish eventalign. Running dataprep over these outputs gives us eventalign.hdf5 which contains all the reads aligned to each position in our reference, and the inference folder which contains all the DRACH sites that can be used as an input to m6anet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENST00000351111.6_156_TGACT_44.hdf5',\n",
       " 'ENST00000351111.6_706_GGACC_42.hdf5',\n",
       " 'ENST00000477012.5_583_AAACA_4.hdf5',\n",
       " 'ENST00000421763.5_675_GGACT_1.hdf5',\n",
       " 'ENST00000273480.3_1025_TAACT_1.hdf5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../demo_data/inference/\"\n",
    "sites = os.listdir(data_dir)\n",
    "sites[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filenames of the inference files are of the following format: contig_position_5-mer_numreads. Here \"numreads\" refers to the number of reads aligned to that particular contig position. We can use this information to filter out some positions that have very few reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>modification_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000351111.6_156_TGACT_44.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000351111.6_706_GGACC_42.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000477012.5_583_AAACA_4.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000421763.5_675_GGACT_1.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000273480.3_1025_TAACT_1.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fnames  modification_status\n",
       "0  ENST00000351111.6_156_TGACT_44.hdf5                  0.0\n",
       "1  ENST00000351111.6_706_GGACC_42.hdf5                  0.0\n",
       "2   ENST00000477012.5_583_AAACA_4.hdf5                  0.0\n",
       "3   ENST00000421763.5_675_GGACT_1.hdf5                  0.0\n",
       "4  ENST00000273480.3_1025_TAACT_1.hdf5                  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"../sample_labels.csv.gz\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file sample_labels.csv.gz is a csv files with modification status assigned to every site located in the inference folder. We can use this information to train and test our model but first let's filter some positions with very few reads. Generally, the more reads we have, the better it is for our m6anet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>modification_status</th>\n",
       "      <th>n_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000351111.6_156_TGACT_44.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000351111.6_706_GGACC_42.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000477012.5_583_AAACA_4.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000421763.5_675_GGACT_1.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000273480.3_1025_TAACT_1.hdf5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fnames  modification_status  n_samples\n",
       "0  ENST00000351111.6_156_TGACT_44.hdf5                  0.0         44\n",
       "1  ENST00000351111.6_706_GGACC_42.hdf5                  0.0         42\n",
       "2   ENST00000477012.5_583_AAACA_4.hdf5                  0.0          4\n",
       "3   ENST00000421763.5_675_GGACT_1.hdf5                  0.0          1\n",
       "4  ENST00000273480.3_1025_TAACT_1.hdf5                  0.0          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_num_reads(fname):\n",
    "    ''' split the string filename to retrieve the number of reads at the end'''\n",
    "    return int(fname.split(\"_\")[-1].split(\".hdf5\")[0])\n",
    "\n",
    "labels[\"n_samples\"] = labels[\"fnames\"].apply(find_num_reads)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us choose position with at least three reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[labels[\"n_samples\"] >= 3]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with just 48 sites. Now let us split these sites into training, testing, and validation sites based on the transcripts. To do this, we use sklearn GroupKFold, GroupShuffleSplit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 sites for training\n",
      "There are 8 sites for testing\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "all_sites = labels_filtered[\"fnames\"].values\n",
    "all_labels = labels_filtered[\"modification_status\"].values\n",
    "all_transcripts = labels_filtered[\"fnames\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "train_idx, test_idx = next(GroupShuffleSplit(n_splits=5).split(all_sites, all_labels,\n",
    "                                                               groups=all_transcripts))\n",
    "\n",
    "\n",
    "print(\"There are {} sites for training\".format(len(train_idx)))\n",
    "print(\"There are {} sites for testing\".format(len(test_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to compute the normalization factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import *\n",
    "from utils.train_utils import *\n",
    "from utils.model import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean and std for kmer AAACA: 100%|██████████| 4/4 [00:00<00:00, 654.64it/s]\n",
      "Computing mean and std for kmer AAACC: 100%|██████████| 3/3 [00:00<00:00, 387.45it/s]\n",
      "Computing mean and std for kmer AGACC: 100%|██████████| 3/3 [00:00<00:00, 201.73it/s]\n",
      "Computing mean and std for kmer AGACT: 100%|██████████| 1/1 [00:00<00:00, 223.16it/s]\n",
      "Computing mean and std for kmer GAACA: 100%|██████████| 4/4 [00:00<00:00, 842.48it/s]\n",
      "Computing mean and std for kmer GAACC: 100%|██████████| 2/2 [00:00<00:00, 454.84it/s]\n",
      "Computing mean and std for kmer GAACT: 100%|██████████| 3/3 [00:00<00:00, 613.32it/s]\n",
      "Computing mean and std for kmer GGACA: 100%|██████████| 4/4 [00:00<00:00, 524.21it/s]\n",
      "Computing mean and std for kmer GGACC: 100%|██████████| 1/1 [00:00<00:00, 237.65it/s]\n",
      "Computing mean and std for kmer GGACT: 100%|██████████| 7/7 [00:00<00:00, 757.33it/s]\n",
      "Computing mean and std for kmer TAACA: 100%|██████████| 1/1 [00:00<00:00, 242.74it/s]\n",
      "Computing mean and std for kmer TGACA: 100%|██████████| 3/3 [00:00<00:00, 616.69it/s]\n",
      "Computing mean and std for kmer TGACC: 100%|██████████| 3/3 [00:00<00:00, 423.41it/s]\n",
      "Computing mean and std for kmer TGACT: 100%|██████████| 1/1 [00:00<00:00, 228.63it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_data_for_training(\"../demo_data/inference/\", \"../demo_data/demo_train\", \n",
    "                          labels_filtered, train_idx, test_idx,\n",
    "                          n_processes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function prepare_data_for_training computes the normalization for each kmer and save it into ../demo_data/demo_train as norm_constant.csv. We need this within the root_dir directory that we are going to pass as an argument into our training dataset later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processors = 10\n",
    "lr = 0.01\n",
    "n_epochs = 5\n",
    "n_print_iters = 1\n",
    "clip_gradient = 0.25\n",
    "device = 'cpu'\n",
    "\n",
    "train_ds = TrainDS(root_dir=\"../demo_data/demo_train\", mode=\"train\", data_dir=data_dir)\n",
    "test_ds = TrainDS(root_dir=\"../demo_data/demo_train\", mode=\"test\", data_dir=data_dir)\n",
    "\n",
    "train_dl_kwargs = {'shuffle': True, 'num_workers': n_processors, 'collate_fn': custom_collate, \n",
    "                   'batch_size': len(train_ds)}\n",
    "\n",
    "test_dl_kwargs = {'shuffle': False, 'num_workers': n_processors, 'collate_fn': custom_collate, \n",
    "                  'batch_size': len(test_ds)}\n",
    "\n",
    "train_dl = DataLoader(train_ds, **train_dl_kwargs)\n",
    "test_dl = DataLoader(test_ds, **test_dl_kwargs)\n",
    "\n",
    "model = MultiInstanceNNEmbedding(dim_cov=3, embedding_dim=2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to run full batch gradient descent since we do not have that much sample data to play with. We are going to use ADAM optimization algorithm and run gradient descent for 5 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1: 1/1]:  Train loss: 0.7161,  Train accuracy: 0.1250,  ROC AUC: 0.1771,  PR AUC: 0.0734\n",
      "=================================\n",
      "[1: 1/1]:  Test loss: 0.6862,  Test accuracy: 0.6250,  ROC AUC: 0.3333,  PR AUC: 0.2823\n",
      "=================================\n",
      "[2: 1/1]:  Train loss: 0.6160,  Train accuracy: 0.8750,  ROC AUC: 0.2057,  PR AUC: 0.0751\n",
      "=================================\n",
      "[2: 1/1]:  Test loss: 0.6826,  Test accuracy: 0.6250,  ROC AUC: 0.5333,  PR AUC: 0.3879\n",
      "=================================\n",
      "[3: 1/1]:  Train loss: 0.4879,  Train accuracy: 0.8750,  ROC AUC: 0.2114,  PR AUC: 0.0749\n",
      "=================================\n",
      "[3: 1/1]:  Test loss: 0.7763,  Test accuracy: 0.6250,  ROC AUC: 0.5333,  PR AUC: 0.3879\n",
      "=================================\n",
      "[4: 1/1]:  Train loss: 0.4157,  Train accuracy: 0.8750,  ROC AUC: 0.2400,  PR AUC: 0.0772\n",
      "=================================\n",
      "[4: 1/1]:  Test loss: 0.8764,  Test accuracy: 0.6250,  ROC AUC: 0.8000,  PR AUC: 0.7111\n",
      "=================================\n",
      "[5: 1/1]:  Train loss: 0.4011,  Train accuracy: 0.8750,  ROC AUC: 0.4571,  PR AUC: 0.1055\n",
      "=================================\n",
      "[5: 1/1]:  Test loss: 0.8807,  Test accuracy: 0.6250,  ROC AUC: 0.8667,  PR AUC: 0.7639\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "running_train_losses, running_test_losses, test_loss_per_epoch, model_states = fit(model, train_dl, device, opt,\n",
    "                                                                                   criterion=torch.nn.BCELoss, \n",
    "                                                                                   n_epochs=n_epochs,\n",
    "                                                                                   test_dl=test_dl, \n",
    "                                                                                   clip_gradient=clip_gradient, \n",
    "                                                                                   n_print_iters=n_print_iters,\n",
    "                                                                                   save_dir=\"../demo_data/results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_states.pt\n",
      "test_loss.pt\n",
      "train_loss.pt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ../demo_data/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The save directory will contain the train and test losses. They are also returned by the fit function as running_train_losses and running_test_losses. The model states per epoch is returned as model_states variable or will be saved into save_dir as well when it is supplied. We can then choose which models we want to deploy by imputing the state dict argument into an instance of MultiInstanceNNEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_states[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m6anet]",
   "language": "python",
   "name": "conda-env-m6anet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
